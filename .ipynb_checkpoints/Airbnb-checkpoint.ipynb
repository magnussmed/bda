{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Airbnb New York City 2019\n",
    "## Business Data Analytics, Quantitative Methods and Visualization - Exam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv( \"AB_NYC_2019.csv\" )\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if the attributes types are correct and usefull\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if values contains NaN\n",
    "df.isnull().sum()\n",
    "\n",
    "# As you can see, there are 10052 NaN values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reviews_per_month might contain \"NaN\" and we can't use that and therefore its replaced with a 0.\n",
    "df.fillna( { 'reviews_per_month':0 }, inplace = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Unnecessary columns:\n",
    "host_id is not needed since we have the name.\n",
    "Latitude and longtitude is not necessary due to the fact we already have neighbourgood_group.\n",
    "last_review does not affect prices\n",
    "name does not affect the prices\n",
    "host_name does not affect the prices\n",
    "\"\"\"\n",
    "df.drop( ['id', 'host_id', 'name', 'host_name', 'latitude', 'longitude', 'last_review'], axis = 1, inplace = True )\n",
    "\n",
    "# Checking if they're dropped\n",
    "df.info()\n",
    "\n",
    "# We can see we have 48.895 rows and 12 columns left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks how many rows where the price is above 1000\n",
    "df[df[\"price\"] > 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks how many rows where the price is above 500\n",
    "df[df[\"price\"] > 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks how many rows where the price is above 300\n",
    "df[df[\"price\"] > 300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks how many rows where the price is above 200\n",
    "df[df[\"price\"] > 200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only use rows if the price is below 300\n",
    "df = df[df[\"price\"] < 300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['minimum_nights'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is likely to assume that 1250 minimum_nights will influence our model\n",
    "df[\"minimum_nights\"][df[\"minimum_nights\"] > 25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"minimum_nights\"][df[\"minimum_nights\"] > 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"minimum_nights\"][df[\"minimum_nights\"] > 35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're now setting every value in minimum_nights that is above 35 to 35.\n",
    "df.loc[(df.minimum_nights > 35),\"minimum_nights\"] = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking unique neighbourhoods\n",
    "df['neighbourhood'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dummies - converts the strings within the DataFrame to integers that can be used for our models\n",
    "df_test = df.drop( ['neighbourhood'], axis = 1 )\n",
    "df      = pd.get_dummies( df )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing a new heatmap with our dummy values. We have left our neighbourhood on purpose due to the fact it contains way too many data points\n",
    "fig, ax = plt.subplots( figsize = (20, 10) )\n",
    "sns.color_palette( \"husl\", 8 )\n",
    "sns.heatmap( df_test.corr(), annot = True, linewidths = .5, ax = ax )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The price column is our target\n",
    "X = df.loc[:, df.columns != 'price']\n",
    "y = df['price']\n",
    "\n",
    "# Split the wave dataset into a training and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, random_state = 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knreg = KNeighborsRegressor().fit( X_train, y_train )\n",
    "print( \"Training set score: {:.3f}\".format( knreg.score( X_train, y_train ) ) )\n",
    "print( \"Test set score: {:.3f}\".format( knreg.score( X_test, y_test ) ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNeighborsRegressor (tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbours      = [85, 90, 95]\n",
    "distances       = ['uniform', 'distance']\n",
    "accuracy_output = []\n",
    "training_scores = []\n",
    "test_scores     = []\n",
    "\n",
    "for neighbour in neighbours: \n",
    "    for distance in distances:\n",
    "        knreg = KNeighborsRegressor( n_neighbors = neighbour ) \n",
    "        knreg.fit( X_train, y_train )\n",
    "        training = knreg.score( X_train, y_train )\n",
    "        tests    = knreg.score( X_test, y_test )\n",
    "        accuracy_output.append( (neighbour, distance, tests, training) )\n",
    "\n",
    "for idx, key in enumerate( sorted ( accuracy_output, reverse = True, key = lambda e:e[2] ) ):\n",
    "    print(f\"Neighbours {key[0]}, weight used {key[1]}, test accuracy: {key[2]}, train accuracy: {key[3]} \\n\")\n",
    "    \n",
    "    if idx == 0:\n",
    "        training_scores.append( key[3] )\n",
    "        test_scores.append( key[2] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression().fit( X_train, y_train )\n",
    "training_score = lr.score( X_train, y_train )\n",
    "test_score = lr.score( X_test, y_test )\n",
    "\n",
    "print( \"Training set score: {:.2f}\".format( training_score ) )\n",
    "print( \"Test set score: {:.2f}\".format( test_score ) )\n",
    "# Might be underfitted, or data just doesn't allow for better accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge().fit( X_train, y_train )\n",
    "print( \"Training set score: {:.3f}\".format( ridge.score( X_train, y_train ) ) )\n",
    "print( \"Test set score: {:.3f}\".format( ridge.score( X_test, y_test ) ) )\n",
    "# Might be underfitted, or data just doesn't allow for better accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge (tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Different inputs tried:\n",
    "fit_int = [True, False]\n",
    "normalize = [True, False]\n",
    "alpha = [0.1, 1, 10, 100]\n",
    "alpha = [1, 5, 10, 15]\n",
    "\"\"\"\n",
    "alpha           = [1, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "accuracy_output = []\n",
    "\n",
    "for a in alpha:\n",
    "    ridge = Ridge( alpha = a ) \n",
    "    ridge.fit( X_train, y_train )\n",
    "    training = ridge.score( X_train, y_train )\n",
    "    tests    = ridge.score( X_test, y_test )\n",
    "    accuracy_output.append( (a, tests, training) )\n",
    "\n",
    "for idx, key in enumerate( sorted ( accuracy_output, reverse = True, key = lambda e:e[1] ) ):\n",
    "    print(f\"alpha: {key[0]}, test accuracy: {key[1]}, train accuracy: {key[2]} \\n\")\n",
    "    \n",
    "    if idx == 0:\n",
    "        training_scores.append( key[2] )\n",
    "        test_scores.append( key[1] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = Lasso().fit( X_train, y_train )\n",
    "print( \"Training set score: {:.3f}\".format( lasso.score( X_train, y_train ) ) )\n",
    "print( \"Test set score: {:.3f}\".format( lasso.score( X_test, y_test ) ) )\n",
    "# print(\"Number of features used:\", np.sum(lasso.coef_ != 0))\n",
    "# Might be underfitted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso (tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alpha = [0.0001, 0.001, 0.01, 0.1, 1.0]\n",
    "# max_iter = [1000, 2000, 3000, 4000]\n",
    "alpha           = [0.0001, 0.001]\n",
    "accuracy_output = []\n",
    "\n",
    "for a in alpha:\n",
    "    lasso = Lasso( alpha = a, max_iter = 50000 ) \n",
    "    lasso.fit( X_train, y_train )\n",
    "    training = lasso.score( X_train, y_train )\n",
    "    tests    = lasso.score( X_test, y_test )\n",
    "    accuracy_output.append( (a, tests, training) )\n",
    "\n",
    "for idx, key in enumerate( sorted ( accuracy_output, reverse = True, key = lambda e:e[1] ) ):\n",
    "    print(f\"alpha: {key[0]}, test accuracy: {key[1]}, train accuracy: {key[2]} \\n\")\n",
    "    \n",
    "    if idx == 0:\n",
    "        training_scores.append( key[2] )\n",
    "        test_scores.append( key[1] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeRegressor().fit( X_train, y_train )\n",
    "print( \"Training set score: {:.3f}\".format( tree.score( X_train, y_train ) ) )\n",
    "print( \"Test set score: {:.3f}\".format( tree.score( X_test, y_test ) ) )\n",
    "# Overfitted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DecisionTreeRegressor (tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth         = [9, 10, 11, 12, 13, 14]\n",
    "min_samples_split = [10, 200, 225, 250, 275, 300]\n",
    "max_features      = ['auto', 'sqrt', 'log2']\n",
    "accuracy_output   = []\n",
    "\n",
    "for md in max_depth:\n",
    "    for mss in min_samples_split:\n",
    "        for mx in max_features:        \n",
    "            tree = DecisionTreeRegressor( max_depth = md, min_samples_split = mss ) \n",
    "            tree.fit( X_train, y_train )\n",
    "            training = tree.score( X_train, y_train )\n",
    "            tests    = tree.score( X_test, y_test )\n",
    "            accuracy_output.append( (md, mss, mx, tests, training) )\n",
    "\n",
    "for idx, key in enumerate( sorted ( accuracy_output, reverse = True, key = lambda e:e[3] ) ):\n",
    "    print(f\"max depth: {key[0]}, min samples split: {key[1]}, max features {key[2]}, test accuracy: {key[3]}, train accuracy: {key[4]} \\n\")\n",
    "    \n",
    "    if idx == 0:\n",
    "        training_scores.append( key[4] )\n",
    "        test_scores.append( key[3] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfr = RandomForestRegressor().fit( X_train, y_train )\n",
    "print( \"Training set score: {:.3f}\".format( rfr.score( X_train, y_train ) ) )\n",
    "print( \"Test set score: {:.3f}\".format( rfr.score( X_test, y_test ) ) )\n",
    "# Might be overfitted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestRegressor (tuning tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "max_depth         = [10, 50, 100]\n",
    "n_estimators      = [300, 400, 500]\n",
    "min_samples_split = [10, 200, 300]\n",
    "max_features      = ['auto', 'sqrt', 'log2']\n",
    "accuracy_output   = []\n",
    "\n",
    "for md in max_depth:\n",
    "    for ne in n_estimators:\n",
    "        for mss in min_samples_split:\n",
    "            for mx in max_features:        \n",
    "                rfr = RandomForestRegressor( max_depth = md, n_estimators = ne, min_samples_split = mss ) \n",
    "                rfr.fit( X_train, y_train )\n",
    "                training = rfr.score( X_train, y_train )\n",
    "                tests = rfr.score( X_test, y_test )\n",
    "                accuracy_output.append( (md, ne, mss, mx, tests, training) )\n",
    "for key in sorted ( accuracy_output, reverse = True, key = lambda e:e[4] ):\n",
    "    print(f\"max depth: {key[0]}, n_estimators: {key[1]} min samples split: {key[2]}, max features {key[3]}, test accuracy: {key[4]}, train accuracy: {key[5]} \\n\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestRegressor (tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal tuning found through manual trial and error\n",
    "rfr = RandomForestRegressor( max_depth = 100, n_estimators = 550, min_samples_split = 50 ) \n",
    "rfr.fit( X_train, y_train )\n",
    "\n",
    "train_score = rfr.score( X_train, y_train )\n",
    "test_score  = rfr.score( X_test, y_test )\n",
    "\n",
    "print( \"Training set score: {:.3f}\".format( train_score ) )\n",
    "print( \"Test set score: {:.3f}\".format( test_score ) )\n",
    "\n",
    "training_scores.append( train_score )\n",
    "test_scores.append( test_score )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestRegressor Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features    = df_test.columns\n",
    "importances = rfr.feature_importances_\n",
    "indices     = np.argsort( importances )\n",
    "\n",
    "plt.subplots( figsize = (20, 20) )\n",
    "plt.title( 'Feature Importances' )\n",
    "plt.barh( range( len( indices ) ), importances[indices], color = 'g', align = 'center' )\n",
    "plt.yticks( range( len( indices ) ), features[indices] )\n",
    "plt.xlabel( 'Importance' )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test and training results visualized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index     = np.arange( 6 )\n",
    "bar_width = 0.5\n",
    "\n",
    "fig, ax  = plt.subplots( figsize = (15, 10) )\n",
    "training = ax.bar( index, training_scores, bar_width, label = \"Training accuracy\" )\n",
    "testing  = ax.bar( index + bar_width, test_scores, bar_width, label = \"Test accuracy\")\n",
    "\n",
    "ax.set_xlabel( 'Models' )\n",
    "ax.set_ylabel( 'Accuracy' )\n",
    "ax.set_title( 'Comparing our models' )\n",
    "ax.set_xticks( index + bar_width / 2 )\n",
    "ax.set_xticklabels( [\"KNeighborsRegressor\", \"LinearRegression\", \"Ridge\", \"Lasso\", \"DecisionTreeRegressor\", \"RandomForestRegressor\"] )\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
